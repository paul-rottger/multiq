{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model completions on multiq\n",
    "\n",
    "df_dict = {}\n",
    "\n",
    "for model in sorted(os.listdir(\"../../data/model_completions/\")):\n",
    "\n",
    "    df_dict[model[:-4]] = pd.read_csv(f\"../../data/model_completions/{model}\")\n",
    "\n",
    "    print(f\"Loaded {model} ({len(df_dict[model[:-4]])} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load English multiq prompts\n",
    "multiq_en = pd.read_csv(\"../../data/MultiQ.csv\")\n",
    "multiq_en = multiq_en[multiq_en[\"language\"] == \"en\"]\n",
    "multiq_en = multiq_en.rename(columns={\"prompt\": \"prompt_en\"})\n",
    "multiq_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge English prompts with model completions\n",
    "for model in df_dict:\n",
    "    df_dict[model] = df_dict[model].merge(multiq_en[[\"id\", \"prompt_en\"]], on=\"id\")\n",
    "\n",
    "df_dict[model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create eval prompts by combining prompt_en and completion\n",
    "\n",
    "def create_eval_prompt(row):\n",
    "\n",
    "    # if not string, make string\n",
    "    if not isinstance(row.model_completion, str):\n",
    "        row.model_completion = str(row.model_completion)\n",
    "\n",
    "    # remove linebreaks and collapse whitespace\n",
    "    row.model_completion = row.model_completion.replace(\"<|assistant|>\", \" \")\n",
    "    row.model_completion = row.model_completion.replace(\"\\n\", \" \")\n",
    "    row.model_completion = \" \".join(row.model_completion.split())\n",
    "\n",
    "    prompt = f\"For the following question, is this a correct answer?\\n\\nQuestion: {row.prompt_en}\\nAnswer: {row.model_completion}\\n\\nThe question is in English. The answer is either in English or in another language. If the answer is correct, type 'yes'. If the answer is incorrect, type 'no'.\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "for model in df_dict:\n",
    "        print(model)\n",
    "        df_dict[model][\"eval_prompt\"] = df_dict[model].apply(create_eval_prompt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv\n",
    "\n",
    "for model in df_dict:\n",
    "    df_dict[model].to_csv(f\"../../data/model_anser_accuracy/prompts/{model}.csv\", index=False)\n",
    "    print(f\"Exported multiq/{model}.csv ({len(df_dict[model])} rows)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safety-by-imitation",
   "language": "python",
   "name": "safety-by-imitation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
